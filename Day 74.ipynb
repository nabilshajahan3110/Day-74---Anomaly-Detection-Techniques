{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "39340746-4dc3-447d-8475-04611227bd38",
   "metadata": {},
   "source": [
    "# Anomaly Detection Techniques in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32e85cc3-9150-4e20-86a5-b779896f4b73",
   "metadata": {},
   "source": [
    "## Objective:\n",
    "The goal of this assignment is to implement and understand various anomaly detection techniques in Python using different datasets and methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6174d227-d2aa-42d4-8e84-9a7e7c68b1ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import sys\n",
    "if not sys.warnoptions:\n",
    "    warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6772ba33-87b3-4d99-a856-517b217daabd",
   "metadata": {},
   "source": [
    "### Task 1: Statistical Anomaly Detection\n",
    "1. **Objective**: Detect anomalies in a numeric dataset using Z-Score and IQR methods.\n",
    "2. **Instructions**:\n",
    "   - Create a dataset with numeric values (e.g., [10, 12, 14, 10, 13, 100, 11]).\n",
    "   - Use Z-Score to identify anomalies (threshold: Z > 3 or Z < -3).\n",
    "   - Use the IQR method to identify anomalies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "da078805-bd51-4530-be29-cf170e74dd73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anomalies using Z-Score: []\n",
      "Anomalies using IQR: [100]\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import zscore\n",
    "import numpy as np\n",
    "\n",
    "# Dataset\n",
    "data = np.array([10, 12, 14, 10, 13, 100, 11])\n",
    "\n",
    "# Z-Score Method\n",
    "z_scores = zscore(data)\n",
    "anomalies_zscore = data[np.abs(z_scores) > 3]\n",
    "print(\"Anomalies using Z-Score:\", anomalies_zscore)\n",
    "\n",
    "# IQR Method\n",
    "q1, q3 = np.percentile(data, [25, 75])\n",
    "iqr = q3 - q1\n",
    "lower_bound, upper_bound = q1 - 1.5 * iqr, q3 + 1.5 * iqr\n",
    "anomalies_iqr = data[(data < lower_bound) | (data > upper_bound)]\n",
    "print(\"Anomalies using IQR:\", anomalies_iqr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ca4457d-f5cc-4217-8e29-189f5eb32a2f",
   "metadata": {},
   "source": [
    "### Task 2: Machine Learning-Based Anomaly Detection\n",
    "1. **Objective**: Detect anomalies using K-Nearest Neighbors (KNN) and Isolation Forest.\n",
    "2. **Instructions**:\n",
    "   - Use the dataset: `[[10], [12], [14], [10], [13], [100], [11]]`.\n",
    "   - Use `sklearn.neighbors.LocalOutlierFactor` for KNN-based anomaly detection.\n",
    "   - Use `sklearn.ensemble.IsolationForest` for Isolation Forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "31469a7b-67e1-423c-aec1-744c1ef8ab2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anomalies using KNN: [[100]]\n",
      "Anomalies using Isolation Forest: [[100]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "# Dataset\n",
    "data = np.array([[10], [12], [14], [10], [13], [100], [11]])\n",
    "\n",
    "# KNN Method\n",
    "lof = LocalOutlierFactor(n_neighbors=2)\n",
    "anomalies_knn = lof.fit_predict(data)\n",
    "print(\"Anomalies using KNN:\", data[anomalies_knn == -1])\n",
    "\n",
    "# Isolation Forest\n",
    "iso_forest = IsolationForest(contamination=0.1)\n",
    "iso_forest.fit(data)\n",
    "anomalies_iso = iso_forest.predict(data)\n",
    "print(\"Anomalies using Isolation Forest:\", data[anomalies_iso == -1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3a016b3-1424-4cba-9360-b9fad5e7a40b",
   "metadata": {},
   "source": [
    "### Task 3: Deep Learning-Based Anomaly Detection\n",
    "1. **Objective**: Use an Autoencoder for anomaly detection.\n",
    "2. **Instructions**:\n",
    "   - Generate a synthetic dataset with normal and anomalous data points.\n",
    "   - Build and train an Autoencoder neural network to reconstruct the input data.\n",
    "   - Identify anomalies using reconstruction error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4c3ce399-db8c-44ad-9981-0a032d2caeeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 116ms/step\n",
      "Anomalies using Autoencoder: [[10.83741291 10.5228392  13.18215125 13.53237863 10.15793072 14.68106123\n",
      "  10.25985642 12.70648168 13.5453026  14.35484562]\n",
      " [13.57043466 14.00864042 11.69725096 14.07412557 10.40057423 14.47408328\n",
      "  12.73796188 14.08648885 12.26159142 13.21788848]\n",
      " [12.6320133  13.65794761 10.40814991 10.30176042 11.23551617 10.7977234\n",
      "  14.35891783 11.09606994 14.87932628 11.68447896]\n",
      " [10.91058958 13.94849254 13.29353888 12.49097858 12.77681775 13.59600889\n",
      "  11.14227371 14.98166958 14.87396581 13.25162843]\n",
      " [10.99771225 13.40114121 10.36099204 10.15326251 11.28841444 12.31311478\n",
      "  14.34136253 13.63584535 13.71353261 12.12746667]]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "import numpy as np\n",
    "\n",
    "# Synthetic Dataset\n",
    "np.random.seed(42)\n",
    "data = np.random.normal(size=(100, 10))\n",
    "data[95:] = np.random.uniform(10, 15, size=(5, 10))  # Add anomalies\n",
    "\n",
    "# Autoencoder\n",
    "autoencoder = Sequential([\n",
    "    Dense(10, activation='relu', input_shape=(10,)),\n",
    "    Dense(5, activation='relu'),\n",
    "    Dense(10, activation='relu')\n",
    "])\n",
    "autoencoder.compile(optimizer='adam', loss='mse')\n",
    "autoencoder.fit(data, data, epochs=20, batch_size=10, verbose=0)\n",
    "\n",
    "# Reconstruction Error\n",
    "reconstructions = autoencoder.predict(data)\n",
    "reconstruction_errors = np.mean((reconstructions - data) ** 2, axis=1)\n",
    "threshold = np.percentile(reconstruction_errors, 95)\n",
    "anomalies_dl = data[reconstruction_errors > threshold]\n",
    "print(\"Anomalies using Autoencoder:\", anomalies_dl)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a6423ed-2a8f-4849-b94c-b144425b391d",
   "metadata": {},
   "source": [
    "### Task 4: Clustering-Based Anomaly Detection\n",
    "1. **Objective**: Detect anomalies using DBSCAN.\n",
    "2. **Instructions**:\n",
    "   - Use the dataset: `[[10], [12], [14], [10], [13], [100], [11]]`.\n",
    "   - Apply DBSCAN clustering and identify outliers (labeled as -1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e2468c58-4d6d-411a-968c-13da30bfb814",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anomalies using DBSCAN: [[100]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "# Dataset\n",
    "data = np.array([[10], [12], [14], [10], [13], [100], [11]])\n",
    "\n",
    "# DBSCAN\n",
    "dbscan = DBSCAN(eps=5, min_samples=2)\n",
    "labels = dbscan.fit_predict(data)\n",
    "anomalies_dbscan = data[labels == -1]\n",
    "print(\"Anomalies using DBSCAN:\", anomalies_dbscan)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08e81ac5-ec5d-48b0-a7dc-18af3c145058",
   "metadata": {},
   "source": [
    "### Task 5: Time-Series Anomaly Detection\n",
    "1. **Objective**: Detect anomalies in a time-series dataset using ARIMA.\n",
    "2. **Instructions**:\n",
    "   - Generate a time-series dataset with anomalies.\n",
    "   - Use ARIMA to model the data and detect anomalies as deviations from the predicted values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ddb77751-e8f0-4573-a878-1b260dbb3ee0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anomalies in Time-Series: 5    120\n",
      "6     12\n",
      "7     14\n",
      "8     13\n",
      "9     10\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "import pandas as pd\n",
    "\n",
    "# Synthetic Time-Series Data\n",
    "data = pd.Series([10, 12, 14, 13, 15, 120, 12, 14, 13, 10])\n",
    "\n",
    "# ARIMA Model\n",
    "model = ARIMA(data, order=(1, 1, 1))\n",
    "model_fit = model.fit()\n",
    "forecast = model_fit.predict(start=0, end=len(data)-1)\n",
    "anomalies_ts = data[(data - forecast).abs() > 10]  # Threshold\n",
    "print(\"Anomalies in Time-Series:\", anomalies_ts)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
